{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = [\n",
    "    \"أ\",\n",
    "    \"ب\",\n",
    "    \"ت\",\n",
    "    \"ث\",\n",
    "    \"ج\"\n",
    "]\n",
    "len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "h, w = 128, 128\n",
    "mean = [0.43216, 0.394666, 0.37645]\n",
    "std = [0.22803, 0.22145, 0.216989]\n",
    "\n",
    "transformer = transforms.Compose([\n",
    "    transforms.Resize((w, h)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "testing_model = torchvision.models.vgg16(pretrained=False)\n",
    "num_ftrs = testing_model.classifier[-1].in_features\n",
    "testing_model.classifier[-1] = nn.Linear(num_ftrs, len(actions))\n",
    "\n",
    "best_weights = torch.load(\"semi-final/weights_img_c5_v1.pth\")\n",
    "testing_model.load_state_dict(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "\n",
    "class Predictor:\n",
    "    def __init__(self, model, transformer):\n",
    "        self.model = model\n",
    "        self.transformer = transformer\n",
    "    \n",
    "    def predict(self, image):\n",
    "        image = self.transformer(image)\n",
    "        image = torch.stack([image])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            res = self.model(image)\n",
    "            best_res = res.argmax()\n",
    "        \n",
    "        return best_res.item()\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "class Utils:\n",
    "    hands = mp.solutions.hands.Hands(static_image_mode=False, min_detection_confidence=0.7, min_tracking_confidence=0.7, max_num_hands=1)\n",
    "    mp_hands = mp.solutions.hands\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "    @staticmethod\n",
    "    def get_bbox(img):\n",
    "        image = img.copy()\n",
    "        image  = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = Utils.hands.process(image)\n",
    "        image.flags.writeable = True\n",
    "        image  = cv2.cvtColor(image,cv2.COLOR_RGB2BGR)\n",
    "        Utils.draw_styled_landmarks(image, results)\n",
    "\n",
    "        if results.multi_hand_landmarks is None:\n",
    "            return image, (0, 0, 0, 0)\n",
    "        \n",
    "        h, w, c = image.shape\n",
    "        x_max, x_min, y_max, y_min = 0, w, 0, h\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]\n",
    "        for lm in hand_landmarks.landmark:\n",
    "            x, y = int(lm.x * w), int(lm.y * h)\n",
    "            if x > x_max:\n",
    "                x_max = x\n",
    "            if x < x_min:\n",
    "                x_min = x\n",
    "            if y > y_max:\n",
    "                y_max = y\n",
    "            if y < y_min:\n",
    "                y_min = y\n",
    "\n",
    "        if x_max + 20 < w:\n",
    "            x_max += 20\n",
    "        else:\n",
    "            x_max = w\n",
    "        \n",
    "        if y_max + 20 < h:\n",
    "            y_max += 20\n",
    "        else:\n",
    "            y_max = h\n",
    "        \n",
    "        if x_min - 20 > 0:\n",
    "            x_min -= 20\n",
    "        else:\n",
    "            x_min = 0\n",
    "        \n",
    "        if y_min - 20 > 0:\n",
    "            y_min -= 20\n",
    "        else:\n",
    "            y_min = 0\n",
    "\n",
    "        return image, (x_min, y_min, x_max, y_max) # left, top, right, bot\n",
    "\n",
    "    @staticmethod\n",
    "    def crop_hand(image, bbox, size=(512, 512)):\n",
    "        segmented_image = np.zeros_like(image)\n",
    "        if bbox != (0, 0, 0, 0):\n",
    "            left, top, right, bot = bbox\n",
    "            segmented_image[top:bot, left:right] = image[top:bot, left:right]\n",
    "            image = image[top:bot, left:right]\n",
    "        image = cv2.resize(image, size)\n",
    "        return image, segmented_image\n",
    "    \n",
    "    @staticmethod\n",
    "    def draw_styled_landmarks(image, results):\n",
    "        # Draw right hand connections\n",
    "        if results.multi_hand_landmarks != None:\n",
    "            for handLandmarks in results.multi_hand_landmarks:\n",
    "                Utils.mp_drawing.draw_landmarks(image, handLandmarks, Utils.mp_hands.HAND_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Final Real-time**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Make decession while processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import operator\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "\n",
    "class LettersPredictor:\n",
    "    def __init__(self, predictor, actions):\n",
    "        self.predictor = predictor\n",
    "        self.actions = actions\n",
    "        self.letters = []\n",
    "        self.predictions = []\n",
    "        self.words = []\n",
    "        self.predictions_count = dict()\n",
    "        \n",
    "    def predict(self, frame):\n",
    "        drawed_landmarks, bbox = Utils.get_bbox(frame)\n",
    "        cropped_img, segmented_image = Utils.crop_hand(frame, bbox)\n",
    "        if bbox != (0, 0, 0, 0):\n",
    "            sign_idx = self.predictor.predict(Image.fromarray(cropped_img))\n",
    "            if sign_idx in self.predictions_count:\n",
    "                self.predictions_count[sign_idx] += 1\n",
    "            else:\n",
    "                self.predictions_count[sign_idx] = 0\n",
    "        \n",
    "            return True, drawed_landmarks, cropped_img, segmented_image\n",
    "        return False, drawed_landmarks, cropped_img, segmented_image\n",
    "    \n",
    "    def compine_letters(self):\n",
    "        word = \"\"\n",
    "        for letter in self.letters:\n",
    "            word += letter\n",
    "        if len(word) > 0:\n",
    "            word = arabic_reshaper.reshape(word)\n",
    "            word = get_display(word) \n",
    "            self.words.append(word)\n",
    "            \n",
    "        self.letters.clear()\n",
    "        self.predictions_count.clear()\n",
    "        return self.words, word\n",
    "    \n",
    "    def predict_letter(self):\n",
    "        if len(self.predictions_count) > 0:\n",
    "                sign_idx = max(self.predictions_count.items(), key=operator.itemgetter(1))[0]\n",
    "                self.predictions.append(sign_idx)\n",
    "                self.predictions = self.predictions[-16:]\n",
    "        if len(self.predictions_count) > 0 and np.unique(self.predictions[-2:])[0] == sign_idx:\n",
    "            if len(self.letters) > 0 and self.actions[sign_idx] != self.letters[-1]:\n",
    "                self.letters.append(self.actions[sign_idx])\n",
    "            else:\n",
    "                self.letters.append(self.actions[sign_idx])\n",
    "        elif len(self.predictions_count) > 0:\n",
    "            self.letters.append(self.actions[sign_idx])        \n",
    "        self.predictions_count.clear()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "import cv2\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "\n",
    "predictor = Predictor(testing_model, transformer)\n",
    "letters_predictor = LettersPredictor(predictor, actions)\n",
    "\n",
    "fontpath = \"arial.ttf\" # <== https://www.freefontspro.com/14454/arial.ttf\n",
    "font = ImageFont.truetype(fontpath, 32)\n",
    "counter = 0\n",
    "discarded_frames = 0\n",
    "words = []\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    flag, drawed_landmarks, cropped_img, segmented_image = letters_predictor.predict(frame)\n",
    "    \n",
    "    if flag:\n",
    "        counter += 1\n",
    "        discarded_frames = 0\n",
    "    else:\n",
    "        discarded_frames += 1\n",
    "\n",
    "    if discarded_frames == 6:\n",
    "        discarded_frames = 0\n",
    "        counter = 0\n",
    "        words, word = letters_predictor.compine_letters()\n",
    "\n",
    "    if counter == 16:\n",
    "        counter = 0\n",
    "        letters_predictor.predict_letter()\n",
    "\n",
    "    cv2.rectangle(drawed_landmarks, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "\n",
    "    img_pil = Image.fromarray(drawed_landmarks)\n",
    "    draw = ImageDraw.Draw(img_pil)\n",
    "    draw.text((0, 0), ' '.join(reversed(words)), font = font)\n",
    "    drawed_landmarks = np.array(img_pil)\n",
    "    \n",
    "    cv2.putText(drawed_landmarks, str(counter), (0, 85+1*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (100, 250, 150), 2, cv2.LINE_8)\n",
    "    cv2.putText(drawed_landmarks, str(discarded_frames), (0, 85+2*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (100, 250, 150), 2, cv2.LINE_8)\n",
    "    cv2.imshow(\"Segmentation\", segmented_image)\n",
    "    cv2.imshow(\"Landmarks\", drawed_landmarks)\n",
    "    cv2.imshow(\"Hand\", cropped_img)\n",
    "\n",
    "    if cv2.waitKey(50) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5442513ba35897157162e772485767663a309d0b481473f29ae9b3d4e5ea1118"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('slr_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
