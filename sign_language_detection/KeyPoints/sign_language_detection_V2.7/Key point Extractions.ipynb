{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0c03c2d",
   "metadata": {},
   "source": [
    "# 1 -  install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0bffa97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "import os\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3e681a",
   "metadata": {},
   "source": [
    "# 2 - keypoints extractions and drawing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d45b88",
   "metadata": {},
   "source": [
    "### \n",
    "\n",
    "- link to mediapipe documentation and info about keypoints numbers\n",
    "- https://google.github.io/mediapipe/solutions/hands.html\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- link to mediapipe code for drawing (to draw the point myself)\n",
    "- https://github.com/google/mediapipe/blob/master/mediapipe/python/solutions/drawing_utils.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab75ce58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "pose_selected_landmarks = [\n",
    "    [0,2,5,11,13,15,12,14,16], # responsible for pose \n",
    "    [0,2,4,5,8,9,12,13,16,17,20], # left hand\n",
    "    [0,2,4,5,8,9,12,13,16,17,20], # right hand\n",
    "]\n",
    "\n",
    "\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# holistic model process image and return the results as keypoints\n",
    "def mediapipe_detection(image,model):\n",
    "    image  = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image  = cv2.cvtColor(image,cv2.COLOR_RGB2BGR)\n",
    "    return image,results\n",
    "\n",
    "                \n",
    "                \n",
    "def extract_keypoints(results):\n",
    "    \n",
    "    original_landmarks = [\n",
    "        results.pose_landmarks,\n",
    "        results.left_hand_landmarks,\n",
    "        results.right_hand_landmarks\n",
    "    ]\n",
    "    \n",
    "    outputs = []\n",
    "    for shape in range(3):\n",
    "        if(original_landmarks[shape]):\n",
    "            lis = original_landmarks[shape].landmark\n",
    "            pose = np.array([ [lis[res].x,lis[res].y] for res in pose_selected_landmarks[shape] ]).flatten()\n",
    "        else:\n",
    "            pose = np.zeros(len(pose_selected_landmarks[shape])*2)\n",
    "        outputs.append(pose)\n",
    "    return np.concatenate([outputs[0],outputs[1],outputs[2]])\n",
    "            \n",
    "\n",
    "\n",
    "def draw_landmark_from_results(image,results):\n",
    "    image_rows, image_cols, _ = image.shape\n",
    "    \n",
    "    original_landmarks = [\n",
    "        results.pose_landmarks,\n",
    "        results.left_hand_landmarks,\n",
    "        results.right_hand_landmarks\n",
    "    ]\n",
    "\n",
    "    \n",
    "    for shape in range(3):\n",
    "        if(original_landmarks[shape]):\n",
    "            lis = original_landmarks[shape].landmark\n",
    "            for idx in pose_selected_landmarks[shape]:\n",
    "                point = lis[idx]\n",
    "                landmark_px = mp_drawing._normalized_to_pixel_coordinates(point.x, point.y,\n",
    "                                                           image_cols, image_rows)\n",
    "\n",
    "                cv2.circle(image, landmark_px, 2, (0,0,255),\n",
    "                         4)\n",
    "\n",
    "\n",
    "def draw_landmark_from_array(image,keyPoints):\n",
    "    image_rows, image_cols, _ = image.shape\n",
    "    \n",
    "    \n",
    "    for i in range(len(keyPoints)//2):\n",
    "        x = keyPoints[i*2]\n",
    "        y = keyPoints[i*2+1]\n",
    "        if(x!=0 and y!=0): \n",
    "            landmark_px = mp_drawing._normalized_to_pixel_coordinates(x,y,\n",
    "                                                       image_cols, image_rows)\n",
    "            cv2.circle(image, landmark_px, 2, (0,0,255),\n",
    "                     4)\n",
    "\n",
    "                \n",
    "\n",
    " \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a53451",
   "metadata": {},
   "source": [
    "# 3 - read and process data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dc390a",
   "metadata": {},
   "source": [
    "### 3.1 get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "7604552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(\"..\",\"..\",\"..\",\"data\",'arabic-signs')\n",
    "signs = sorted(os.listdir(data_path))\n",
    "\n",
    "# for 10 signs\n",
    "\n",
    "actions_ids= [\n",
    "    0,1,2,3,4,5,6,7,8,9 # first 10 actions\n",
    "]\n",
    "actions = [\n",
    "    'one','you','teacher','girl','tomorrow','mom','look','crazy','walk','agree'\n",
    "]\n",
    "n_actions = len(actions)\n",
    "\n",
    "def get_one_class(sign_id):\n",
    "    sign_path = os.path.join(data_path,signs[sign_id])\n",
    "    lis = []\n",
    "    for path in os.listdir(sign_path):\n",
    "        lis.append(os.path.join(sign_path,path,\"Color\"))\n",
    "    return lis\n",
    "\n",
    "\n",
    "def get_frames(dir_path):\n",
    "    lis = [] \n",
    "    for frame in os.listdir(dir_path):\n",
    "        lis.append(os.path.join(dir_path,frame))\n",
    "    return lis\n",
    "# np.linspace(0,32,16,dtype=np.int16)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "47dd51c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "train_y = []\n",
    "test_x = []\n",
    "test_y = []\n",
    "\n",
    "for i in range(10):\n",
    "    data = np.array(get_one_class(i))\n",
    "    indices = np.random.permutation(len(data))\n",
    "    spliting = int(0.9*len(data))\n",
    "    train_x.extend([str(i) for i in data[0:spliting] ])\n",
    "    train_y.extend([i for j in range(spliting)])\n",
    "    test_x.extend([str(i) for i in data[spliting:] ])\n",
    "    test_y.extend([i for j in range(len(data)-spliting)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "a1931a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "859 859 99 99\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    len(train_x),\n",
    "    len(train_y),\n",
    "    len(test_x),\n",
    "    len(test_y),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a781311b",
   "metadata": {},
   "source": [
    "### 3.2 get frames from path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "d602a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import random\n",
    "\n",
    "class VideoProcessing:\n",
    "    def __init__(self,num_frames,transformer=None):\n",
    "        self.transformer = transformer # the datagenerator class\n",
    "        self.num_frames = num_frames   # the num_frames per video\n",
    "        self.seed = random.randint(1,100000000)\n",
    "    \n",
    "    \n",
    "    def change_seed(self):\n",
    "        self.seed = random.randint(1,100000000)\n",
    "    \n",
    "    def transform(self,frame):\n",
    "        for trans_frame in self.transformer.flow(np.expand_dims(frame, axis=0),seed=self.seed):\n",
    "            return np.squeeze(trans_frame.astype(np.uint8), axis=0)\n",
    "        \n",
    "        \n",
    "\n",
    "#     def __capture_frames(self,video_path):\n",
    "#         video = cv2.VideoCapture(video_path)\n",
    "#         video_length = int(video.get(cv2.CAP_PROP_FRAME_COUNT)) - 1\n",
    "\n",
    "#         count=0\n",
    "#         frames = []\n",
    "#         while video.isOpened():\n",
    "#             ret, frame = video.read()\n",
    "#             if not ret:\n",
    "#                 continue\n",
    "#             frames.append(frame)\n",
    "#             count += 1\n",
    "#             if (count > (video_length-1)):\n",
    "#                 video.release()\n",
    "#         video.release()\n",
    "#         return np.array(frames)\n",
    "\n",
    "\n",
    "#     def get_frames(self,video_path,num_frames):\n",
    "#         # collect 2 extra frames and remove one in the beginnign and last one\n",
    "#         num_frames+=4\n",
    "        \n",
    "#         frames = self.__capture_frames(video_path)\n",
    "#         video_length = len(frames)\n",
    "#         steps = video_length/num_frames\n",
    "#         count=0\n",
    "#         new_frames=[]\n",
    "#         while count<video_length:\n",
    "#             frame = frames[int(count)]\n",
    "#             if(self.transformer !=None):\n",
    "#                 frame = self.transform(frame)\n",
    "#             new_frames.append(frame)\n",
    "#             count+=steps\n",
    "        \n",
    "        \n",
    "#         num_frames-=4\n",
    "        \n",
    "#         # return np.array(new_frames[:num_frames])\n",
    "    \n",
    "#         return  np.array(new_frames[2:num_frames+2])\n",
    "\n",
    "\n",
    "    def get_frames(self,video_path,num_frames):\n",
    "        lis = [] \n",
    "        video_frames = os.listdir(video_path)\n",
    "        selected_frames = np.linspace(0,len(video_frames)-1,num_frames,dtype=np.int16)\n",
    "        for index in selected_frames:\n",
    "            frame = video_frames[index]\n",
    "            lis.append(cv2.imread(os.path.join(video_path,frame)))\n",
    "        return lis\n",
    "\n",
    "    \n",
    "    def extract_keypoints_video(self,frames=None,path=None,display_text=None):\n",
    "        self.change_seed()\n",
    "        if(display_text != None ):\n",
    "            print(display_text,end=\"\\r\")\n",
    "\n",
    "        if(frames==None):\n",
    "            frames = self.get_frames(path,self.num_frames)\n",
    "            \n",
    "        output_key_points=[]\n",
    "        output_images=[]\n",
    "\n",
    "        for frame in frames:\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "            output_key_points.append(extract_keypoints(results))\n",
    "            output_images.append(image)\n",
    "        return np.array(output_images),np.array(output_key_points)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class VideosProcessing:\n",
    "    def __init__(self,transformer,num_frames):\n",
    "        self.processor = VideoProcessing(transformer=transformer,num_frames=num_frames)\n",
    "        self.num_frames = num_frames\n",
    "        self.transformer = transformer\n",
    "        \n",
    "    def stop_transofrmation(self):\n",
    "        self.processor.transformer = None\n",
    "        \n",
    "    def enable_transformation(self):\n",
    "        self.processor.transformer = self.transformer\n",
    "        \n",
    "        \n",
    "    def convert_get_both(self,array):\n",
    "        output = []\n",
    "        frames_output=[]\n",
    "        for index,video in enumerate(array):\n",
    "            display_text = f\"processing video : {index+1}/{len(array)}\"\n",
    "            frames,keypoints = self.processor.extract_keypoints_video(path=video,display_text=display_text)\n",
    "            output.append(keypoints)\n",
    "            frames_output.append(frames)\n",
    "        return np.array(frames_output),np.array(output)\n",
    "        \n",
    "    def convert(self,array):\n",
    "        output = []\n",
    "        for index,video in enumerate(array):\n",
    "            display_text = f\"processing video : {index+1}/{len(array)}\"\n",
    "            frames,keypoints = self.processor.extract_keypoints_video(path=video,display_text=display_text)\n",
    "            output.append(keypoints)\n",
    "        return np.array(output)\n",
    "\n",
    "    \n",
    "    \n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=10,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        #horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "\n",
    "\n",
    "video_processing_obj = VideoProcessing(transformer=datagen,num_frames=16)\n",
    "video_list_obj = VideosProcessing(transformer=datagen,num_frames=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd4a1c",
   "metadata": {},
   "source": [
    "### 3.3 test extracted images and frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e99b798",
   "metadata": {},
   "source": [
    "#### 3.3.1 get and view keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "ac260a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing video : 3/3\r"
     ]
    }
   ],
   "source": [
    "# one_class = get_one_class(0,\"train\")\n",
    "# video_list_obj.stop_transofrmation()\n",
    "video_list_obj.enable_transformation()\n",
    "frames_list,keypoints_list = video_list_obj.convert_get_both(train_x[30:33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "bc5c38a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing video : 5/5\r"
     ]
    }
   ],
   "source": [
    "data = get_one_class(1,\"test\")[:5] # get 5 videos with class label from training data\n",
    "video_list_obj.stop_transofrmation()\n",
    "frames_list,keypoints_list = video_list_obj.convert_get_both(data) # convert them to kye points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "98ffbfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images,keypoints = video_processing_obj.extract_keypoints_video(path=train_X[250])\n",
    "\n",
    "for video_num in range(len(frames_list)):\n",
    "    images = frames_list[video_num]\n",
    "    keypoints = keypoints_list[video_num]\n",
    "\n",
    "    for index in range(16):\n",
    "\n",
    "        image = images[index]\n",
    "\n",
    "        keypoint = keypoints[index]\n",
    "\n",
    "        draw_landmark_from_array(image,keypoint)\n",
    "\n",
    "        cv2.imshow(\"frame\",image)\n",
    "        if cv2.waitKey(200) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c42b0a",
   "metadata": {},
   "source": [
    "#### 3.3.2 view keypoints only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "ba771efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing video : 5/5\r"
     ]
    }
   ],
   "source": [
    "one_class = get_one_class(5,\"train\")\n",
    "# video_list_obj.stop_transofrmation()\n",
    "video_list_obj.enable_transformation()\n",
    "keypoints_list = video_list_obj.convert(one_class[60:62])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "1596c1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images,keypoints = video_processing_obj.extract_keypoints_video(path=train_X[250])\n",
    "\n",
    "for keypoints in keypoints_list:\n",
    "\n",
    "    for index in range(16):\n",
    "\n",
    "        image = np.zeros((512,512,3))+255\n",
    "\n",
    "        keypoint = keypoints[index]\n",
    "\n",
    "        draw_landmark_from_array(image,keypoint)\n",
    "\n",
    "        cv2.imshow(\"frame\",image)\n",
    "        if cv2.waitKey(200) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bdc676",
   "metadata": {},
   "source": [
    "#### 3.3.3 view keypoints from numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "08494a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fil_keypoints(array):\n",
    "    output = array.copy()\n",
    "    for i in range(2,len(output)):\n",
    "        current_frame = output[i]\n",
    "        prev_prev_frame = output[i-2]\n",
    "        prev_frame = output[i-1]\n",
    "        for index,num in enumerate(current_frame):\n",
    "            if num==0:\n",
    "                current_frame[index] = prev_frame[index]*2 - prev_prev_frame[index]\n",
    "                \n",
    "    return output\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "898fe74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keypoints_list = np.load(os.path.join(\"key_points\",\"val\",'1.npy'))\n",
    "\n",
    "test_path,_ =  get_list(actions_ids,\"val\")\n",
    "\n",
    "keypoints_list = val_X[:15] # use thing after loading text_X from keypoints directory\n",
    "new_video_processing = VideoProcessing(transformer=None,num_frames=16)\n",
    "for video_index,keypoints in enumerate(keypoints_list):\n",
    "    images = new_video_processing.get_frames(test_path[video_index],16)\n",
    "    new_keypoints = fil_keypoints(keypoints)\n",
    "    for index in range(16):\n",
    "\n",
    "        image = images[index]\n",
    "\n",
    "        keypoint = new_keypoints[index]\n",
    "\n",
    "        draw_landmark_from_array(image,keypoint)\n",
    "\n",
    "        cv2.imshow(\"frame\",image)\n",
    "        if cv2.waitKey(200) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "4960940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_list = np.load(os.path.join(\"key_points\",\"val\",'1.npy'))\n",
    "\n",
    "for keypoints in keypoints_list:\n",
    "\n",
    "    for index in range(16):\n",
    "\n",
    "        image = np.zeros((512,512,3))+255\n",
    "\n",
    "        keypoint = keypoints[index]\n",
    "\n",
    "        draw_landmark_from_array(image,keypoint)\n",
    "\n",
    "        cv2.imshow(\"frame\",image)\n",
    "        if cv2.waitKey(200) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0a2bce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4424e1",
   "metadata": {},
   "source": [
    "# 4 - extract keypoint and save them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da5f753",
   "metadata": {},
   "source": [
    "### 4.1  extract training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "16b5ad3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 0                                         \n",
      "iteration : 1                                         \n",
      "iteration : 2                                         \n",
      "iteration : 3                                         \n",
      "iteration : 4                                         \n",
      "iteration : 5                                         \n",
      "processing video : 859/859\r"
     ]
    }
   ],
   "source": [
    "# collect realdata and 5 different transformations\n",
    "\n",
    "label_path = os.path.join(\"key_points\",\"train\",\"labels.npy\")\n",
    "np.save(label_path,train_y)\n",
    "\n",
    "\n",
    "num_training_iterations = 6\n",
    "\n",
    "for transformation_index in range(num_training_iterations):\n",
    "    data_path = os.path.join(\"key_points\",\"train\",str(transformation_index)+\".npy\")\n",
    "    \n",
    "    print(\"iteration :\",transformation_index,\" \"*40)\n",
    "    \n",
    "    if(transformation_index == 0):\n",
    "        video_list_obj.stop_transofrmation()\n",
    "    else:\n",
    "        video_list_obj.enable_transformation()\n",
    "    \n",
    "    \n",
    "    data = train_x  # get videos with class label from training data\n",
    "    data = video_list_obj.convert(data) # convert them to kye points\n",
    "    np.save(data_path,data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb495eb",
   "metadata": {},
   "source": [
    "### 4.2  extract testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "7118b981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing video : 99/99\r"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join(\"key_points\",\"test\",\"data.npy\")\n",
    "label_path = os.path.join(\"key_points\",\"test\",\"labels.npy\")\n",
    "\n",
    "# save labels\n",
    "np.save(label_path,test_y)\n",
    "\n",
    "# save data\n",
    "video_list_obj.stop_transofrmation()\n",
    "data = test_x  # get videos with class label from training data\n",
    "data = video_list_obj.convert(data) # convert them to kye points\n",
    "np.save(data_path,data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067d55d0",
   "metadata": {},
   "source": [
    "### 4.3 load and test all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0e8edd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# both depends on actions_id -> [0,1,2,3,4,5,6,7,8,9]\n",
    "def load_dir(dir_name,data_temp=None,labels_temp=None,actions_id=None):\n",
    "    if actions_id == None or action_id == \"all\":\n",
    "        actions_id = [int(s.split('.')[0]) for s in os.listdir(os.path.join(dir_name))]\n",
    "        actions_id.sort()\n",
    "    for action_id in actions_id:\n",
    "        new_array = np.load(os.path.join(dir_name,f\"{action_id}.npy\"))\n",
    "        labels_array = np.array([action_id]*len(new_array))\n",
    "\n",
    "        if(type(data_temp) == np.ndarray):\n",
    "            data_temp = np.concatenate([data_temp,new_array])\n",
    "            labels_temp = np.concatenate([labels_temp,labels_array])\n",
    "        else:\n",
    "            data_temp = new_array\n",
    "            labels_temp = labels_array\n",
    "    \n",
    "    return data_temp,labels_temp\n",
    "\n",
    "def load_mul_dir(parent_dir):\n",
    "    data_temp = None\n",
    "    labels_temp = None\n",
    "    for transformation_index in range(len(os.listdir(parent_dir))):\n",
    "        dir_name = os.path.join(parent_dir,str(transformation_index))\n",
    "        data_temp,labels_temp = load_dir(dir_name,data_temp,labels_temp)\n",
    "    return data_temp,labels_temp\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b6ed89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,train_Y = load_mul_dir(os.path.join(\"key_points\",\"train\"))\n",
    "val_X,val_Y = load_dir(os.path.join(\"key_points\",\"val\"))\n",
    "test_X,test_Y = load_dir(os.path.join(\"key_points\",\"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e1b1216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7446, 16, 62) (7446,) (190, 16, 62) (190,) (168, 16, 62) (168,)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "train_X.shape,\n",
    "    train_Y.shape,\n",
    "    val_X.shape,\n",
    "    val_Y.shape,\n",
    "    test_X.shape,\n",
    "    test_Y.shape\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "0f0f8f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test video on train_x\n",
    "keypoints_list = train_x\n",
    "for keypoints in keypoints_list:\n",
    "\n",
    "    for index in range(16):\n",
    "\n",
    "        image = np.zeros((512,512,3))+255\n",
    "\n",
    "        keypoint = keypoints[index]\n",
    "\n",
    "        draw_landmark_from_array(image,keypoint)\n",
    "\n",
    "        cv2.imshow(\"frame\",image)\n",
    "        if cv2.waitKey(200) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de803f4",
   "metadata": {},
   "source": [
    "# extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f76327",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = np.random.permutation(len(train_data))\n",
    "\n",
    "train_X = train_data[perm]\n",
    "train_Y = train_labels[perm]\n",
    "val_X = val_data\n",
    "val_Y = val_labels\n",
    "test_X = test_data\n",
    "test_Y = test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "10087b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4482, 20, 62) (4482,) (118, 20, 62) (118,) (100, 20, 62) (100,)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "train_X.shape,\n",
    "train_Y.shape,\n",
    "val_X.shape,\n",
    "val_Y.shape,\n",
    "test_X.shape,\n",
    "test_Y.shape,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
