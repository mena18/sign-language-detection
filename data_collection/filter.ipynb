{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abcb7bbc",
   "metadata": {},
   "source": [
    "# initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c888ee1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-20 18:27:31.609949: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mina/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-05-20 18:27:31.609964: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "import os\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38f10af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "dir_path = os.path.join(\"data_v2\")\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7632cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image,model):\n",
    "    image  = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image  = cv2.cvtColor(image,cv2.COLOR_RGB2BGR)\n",
    "    return image,results\n",
    "\n",
    "\n",
    "\n",
    "def draw_styled_landmarks(image,results):\n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    \n",
    "\n",
    "\n",
    "def diff(pose,hand):\n",
    "    pose_x,pose_y = pose[0].x,pose[0].y\n",
    "    hand_x,hand_y = hand[0].x,hand[0].y\n",
    "    \n",
    "    return abs(pose_y-hand_y)\n",
    "    #return abs(pose_x-hand_x) + abs(pose_y-hand_y)\n",
    "\n",
    "    \n",
    "    \n",
    "def draw_rect(frame,lis):\n",
    "    min_x = 1000000\n",
    "    max_x=0\n",
    "    min_y=1000000\n",
    "    max_y=0\n",
    "    \n",
    "    for point in lis:\n",
    "        landmark_px = mp_drawing._normalized_to_pixel_coordinates(point.x, point.y,\n",
    "                                                   frame.shape[1], frame.shape[0])\n",
    "        \n",
    "\n",
    "        \n",
    "        if landmark_px:\n",
    "\n",
    "            min_x = min(landmark_px[0],min_x)\n",
    "            min_y = min(landmark_px[1],min_y)\n",
    "            max_x = max(landmark_px[0],max_x)\n",
    "            max_y = max(landmark_px[1],max_y)\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    \n",
    "    Shift_X = frame.shape[1]//50\n",
    "    Shift_Y = frame.shape[0]//50\n",
    "    \n",
    "    min_x = max(0,min_x-Shift_X)\n",
    "    min_y = max(0,min_y-Shift_Y)\n",
    "    max_x = min(frame.shape[0],max_x+Shift_X)\n",
    "    max_y = min(frame.shape[1],max_y+Shift_Y)\n",
    "    \n",
    "    cv2.rectangle(frame,(min_x,min_y),(max_x,max_y),(255,0,0),10)\n",
    "\n",
    "    return True\n",
    "\n",
    "    \n",
    "\n",
    "def write_text(image,text,position,size='l'):\n",
    "    COLOR = (255,255,255)\n",
    "    if size=='l':\n",
    "        cv2.putText(image, text, position, \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, COLOR, 4, cv2.LINE_AA)\n",
    "    elif size=='s':\n",
    "        cv2.putText(image, text, position, \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLOR, 1, cv2.LINE_AA)\n",
    "        \n",
    "        \n",
    "pose_selected_landmarks = [\n",
    "    [0,2,5,11,13,15,12,14,16], # responsible for pose \n",
    "    [0,2,4,5,8,9,12,13,16,17,20], # left hand\n",
    "    [0,2,4,5,8,9,12,13,16,17,20], # right hand\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def draw_landmark_from_results(image,results):\n",
    "    image_rows, image_cols, _ = image.shape\n",
    "    \n",
    "    original_landmarks = [\n",
    "        results.pose_landmarks,\n",
    "        results.left_hand_landmarks,\n",
    "        results.right_hand_landmarks\n",
    "    ]\n",
    "\n",
    "    \n",
    "    for shape in range(3):\n",
    "        if(original_landmarks[shape]):\n",
    "            lis = original_landmarks[shape].landmark\n",
    "            for idx in pose_selected_landmarks[shape]:\n",
    "                point = lis[idx]\n",
    "                landmark_px = mp_drawing._normalized_to_pixel_coordinates(point.x, point.y,\n",
    "                                                           image_cols, image_rows)\n",
    "\n",
    "                cv2.circle(image, landmark_px, 2, (0,0,255),\n",
    "                         4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cc7fa3",
   "metadata": {},
   "source": [
    "# 1 - remove first 5 frames from each video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d59eb570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "family Started\n",
      "current video 49\n",
      "current video 99\n",
      "current video 149\n",
      "current video 199\n",
      "family finished\n",
      "----------------------------------------------------------------------\n",
      "friends Started\n",
      "current video 49\n",
      "current video 99\n",
      "current video 149\n",
      "current video 199\n",
      "friends finished\n",
      "----------------------------------------------------------------------\n",
      "get_to_know Started\n",
      "current video 49\n",
      "current video 99\n",
      "current video 149\n",
      "current video 199\n",
      "get_to_know finished\n",
      "----------------------------------------------------------------------\n",
      "help Started\n",
      "current video 49\n",
      "current video 99\n",
      "current video 149\n",
      "current video 199\n",
      "help finished\n",
      "----------------------------------------------------------------------\n",
      "how_are_you Started\n",
      "current video 49\n",
      "current video 99\n",
      "current video 149\n",
      "current video 199\n",
      "how_are_you finished\n",
      "----------------------------------------------------------------------\n",
      "love Started\n",
      "current video 49\n",
      "current video 99\n",
      "current video 149\n",
      "current video 199\n",
      "love finished\n",
      "----------------------------------------------------------------------\n",
      "street Started\n",
      "current video 49\n",
      "current video 99\n",
      "current video 149\n",
      "current video 199\n",
      "street finished\n",
      "----------------------------------------------------------------------\n",
      "teach_me Started\n",
      "current video 49\n",
      "current video 99\n",
      "current video 149\n",
      "current video 199\n",
      "teach_me finished\n",
      "----------------------------------------------------------------------\n",
      "thank_you Started\n",
      "current video 49\n",
      "current video 99\n",
      "current video 149\n",
      "current video 199\n",
      "thank_you finished\n",
      "----------------------------------------------------------------------\n",
      "want Started\n",
      "current video 49\n",
      "current video 99\n",
      "current video 149\n",
      "current video 199\n",
      "want finished\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "input_dir_path = os.path.join(\"data_v2\")\n",
    "output_dir_path = os.path.join('data_v2_cleaned')\n",
    "# signs = [\"agree\",'crazy','girl','look','mom','teacher','tommorow','walk','you']\n",
    "signs = [\"family\",'friends','get_to_know','help','how_are_you','love','street','teach_me','thank_you','want']\n",
    "\n",
    "for sign in signs:\n",
    "    try:\n",
    "        os.mkdir(os.path.join(output_dir_path,sign))\n",
    "    except:\n",
    "        pass\n",
    "    videos = os.listdir(os.path.join(input_dir_path,sign))\n",
    "    \n",
    "    print(f'{sign} Started')\n",
    "\n",
    "    continue_view = True\n",
    "    frame_num = 0\n",
    "\n",
    "    for video_num,video in enumerate(sorted([int(i.split('.')[0]) for i in videos])):\n",
    "        video = str(video)+'.mp4'\n",
    "        cap = cv2.VideoCapture(os.path.join(input_dir_path,sign,video))\n",
    "        frame_num = 0\n",
    "        out = cv2.VideoWriter(os.path.join(output_dir_path,sign,video),0x7634706d, 25, (640,480))\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            frame_num+=1\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            if frame_num < 6:\n",
    "                continue\n",
    "\n",
    "    #         frame,results = mediapipe_detection(frame,holistic)\n",
    "    #         draw_styled_landmarks(frame,results)\n",
    "    #         write_text(frame,'frame num {}'.format(frame_num),(50,40))\n",
    "            # cv2.imshow('OpenCV Feed', frame)\n",
    "            out.write(frame)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                continue_view = False\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        out.release()\n",
    "\n",
    "        if not continue_view:\n",
    "            break\n",
    "            \n",
    "        if((video_num+1)%50==0):\n",
    "            print(\"current video\",video_num)\n",
    "            \n",
    "    print(f'{sign} finished')\n",
    "    print(\"-\"*70)\n",
    "        \n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7f26896b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7d80e3",
   "metadata": {},
   "source": [
    "# 2 - remove bad signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbce90bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love/105.mp4\n",
      "love/153.mp4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "street/23.mp4\n",
      "street/131.mp4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "teach_me/67.mp4\n",
      "teach_me/103.mp4\n",
      "teach_me/176.mp4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "thank_you/18.mp4\n",
      "thank_you/154.mp4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "want/39.mp4\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "input_dir_path = os.path.join('data_v2_cleaned')\n",
    "# signs = [\"agree\",'crazy','girl','look','mom','teacher','tommorow','walk','you','one']\n",
    "signs = [\"family\",'friends','get_to_know','help','how_are_you','love','street','teach_me','thank_you','want']\n",
    "signs = ['love','street','teach_me','thank_you','want']\n",
    "\n",
    "for sign in signs:\n",
    "\n",
    "    videos = os.listdir(os.path.join(input_dir_path,sign))\n",
    "\n",
    "\n",
    "    continue_view = True\n",
    "    frame_num = 0\n",
    "\n",
    "    for video_num,video in enumerate(sorted([int(i.split('.')[0]) for i in videos])):\n",
    "        video = str(video)+'.mp4'\n",
    "        cap = cv2.VideoCapture(os.path.join(input_dir_path,sign,video))\n",
    "        length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        if length < 25:\n",
    "            print(sign,'/',video,sep=\"\")\n",
    "            continue\n",
    "        frame_num = -1\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            frame_num+=1\n",
    "            if frame_num % 3 !=0:\n",
    "                continue\n",
    "                \n",
    "            \n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            \n",
    "\n",
    "\n",
    "            write_text(frame,'video num {}'.format(video_num),(50,40))\n",
    "            write_text(frame,'frame num {}'.format(frame_num),(50,140))\n",
    "            cv2.imshow('OpenCV Feed', frame)\n",
    "\n",
    "            if cv2.waitKey(20) & 0xFF == ord('q'):\n",
    "                print(sign,'/',video,sep=\"\")\n",
    "                if cv2.waitKey(100) & 0xFF == ord('t'):\n",
    "                    continue_view = False\n",
    "                    break\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        if not continue_view:\n",
    "            break\n",
    "    if not continue_view:\n",
    "            break\n",
    "\n",
    "\n",
    "    print(\"-\"*100)\n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2397305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0e9a22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(os.path.join(input_dir_path,\"thank_you\",\"159.mp4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "825ae520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print( length )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eafd58e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91675312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45c64423",
   "metadata": {},
   "source": [
    "# 2 - get keypoints from Directory\n",
    "\n",
    "- it does exactly as the name suggests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cf1ea116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 \n",
    "import os\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "16dce9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image,model):\n",
    "    image  = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image  = cv2.cvtColor(image,cv2.COLOR_RGB2BGR)\n",
    "    return image,results\n",
    "\n",
    "        \n",
    "    \n",
    "def extract_keypoints(results,status='l'):\n",
    "    if status == 'l':\n",
    "        return np.array([ [res.x,res.y] for res in results.left_hand_landmarks.landmark ]).flatten()\n",
    "    else:\n",
    "        return np.array([ [res.x,res.y] for res in results.right_hand_landmarks.landmark ]).flatten()\n",
    "    \n",
    "                \n",
    "\n",
    "def diff(pose,hand):\n",
    "    pose_x,pose_y = pose[0].x,pose[0].y\n",
    "    hand_x,hand_y = hand[0].x,hand[0].y\n",
    "    \n",
    "    return abs(pose_y-hand_y)\n",
    "    #return abs(pose_x-hand_x) + abs(pose_y-hand_y)\n",
    "\n",
    "\n",
    "    \n",
    "def validate_hand(frame,lis):\n",
    "    \n",
    "    for point in lis:\n",
    "        landmark_px = mp_drawing._normalized_to_pixel_coordinates(point.x, point.y,\n",
    "                                                   frame.shape[1], frame.shape[0])\n",
    "        if not landmark_px:\n",
    "            return False\n",
    "\n",
    "\n",
    "    return True\n",
    "\n",
    "    \n",
    "def add_left(data,frames,frame,results):\n",
    "    condition = validate_hand(frame,results.left_hand_landmarks.landmark)\n",
    "    if condition:\n",
    "        data.append(extract_keypoints(results,'l'))\n",
    "        frames.append(frame)\n",
    "        \n",
    "        \n",
    "\n",
    "def add_right(data,frames,frame,results):\n",
    "    condition = validate_hand(frame,results.right_hand_landmarks.landmark)\n",
    "    if condition:\n",
    "        data.append(extract_keypoints(results,'r'))\n",
    "        frames.append(frame)\n",
    "    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "be700f7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_key_points(images):\n",
    "\n",
    "    data = []\n",
    "    frames = []\n",
    "\n",
    "    for i,image in enumerate(images):\n",
    "        print(\"image num\",i)\n",
    "\n",
    "        frame = cv2.imread(os.path.join(dir_path,image))\n",
    "\n",
    "\n",
    "        frame,results = mediapipe_detection(frame,holistic)\n",
    "\n",
    "\n",
    "        valid_left =  results.left_hand_landmarks\n",
    "        valid_right = results.right_hand_landmarks\n",
    "\n",
    "        if valid_right and valid_left:\n",
    "            left_diff = diff(results.pose_landmarks.landmark,results.left_hand_landmarks.landmark)\n",
    "            right_diff = diff(results.pose_landmarks.landmark,results.right_hand_landmarks.landmark)\n",
    "            if left_diff < right_diff:\n",
    "                add_left(data,frames,frame,results)\n",
    "            else:\n",
    "                add_right(data,frames,frame,results)\n",
    "        elif valid_right:\n",
    "            add_right(data,frames,frame,results)\n",
    "        elif valid_left:\n",
    "            add_left(data,frames,frame,results)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        frame = cv2.resize(frame, (640, 480)) \n",
    "        cv2.imshow(\"frame\",frame)\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return data,frames\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c97e87ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image num 0\n",
      "image num 1\n",
      "image num 2\n",
      "image num 3\n",
      "image num 4\n",
      "image num 5\n",
      "image num 6\n",
      "image num 7\n",
      "image num 8\n",
      "image num 9\n",
      "image num 10\n",
      "image num 11\n",
      "image num 12\n",
      "image num 13\n",
      "image num 14\n",
      "image num 15\n",
      "image num 16\n",
      "image num 17\n",
      "image num 18\n",
      "image num 19\n",
      "image num 20\n",
      "image num 21\n",
      "image num 22\n",
      "image num 23\n",
      "image num 24\n",
      "image num 25\n",
      "image num 26\n",
      "image num 27\n",
      "image num 28\n",
      "image num 29\n",
      "image num 30\n",
      "image num 31\n",
      "image num 32\n",
      "image num 33\n",
      "image num 34\n",
      "image num 35\n",
      "image num 36\n",
      "image num 37\n",
      "image num 38\n",
      "image num 39\n",
      "image num 40\n",
      "image num 41\n",
      "image num 42\n",
      "image num 43\n",
      "image num 44\n",
      "image num 45\n",
      "image num 46\n",
      "image num 47\n",
      "image num 48\n",
      "image num 49\n",
      "image num 50\n",
      "image num 51\n",
      "image num 52\n",
      "image num 53\n",
      "image num 54\n",
      "image num 55\n",
      "image num 56\n",
      "image num 57\n",
      "image num 58\n",
      "image num 59\n",
      "image num 60\n",
      "image num 61\n",
      "image num 62\n",
      "image num 63\n",
      "image num 64\n",
      "image num 65\n",
      "image num 66\n",
      "image num 67\n",
      "image num 68\n",
      "image num 69\n",
      "image num 70\n",
      "image num 71\n",
      "image num 72\n",
      "image num 73\n",
      "image num 74\n"
     ]
    }
   ],
   "source": [
    "dir_path = os.path.join(\"..\",\"..\",\"data\",\"data_letter_khaa\")\n",
    "images = os.listdir(dir_path)\n",
    "\n",
    "data,frames = get_key_points(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b7658382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 72)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data),len(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9828a8b3",
   "metadata": {},
   "source": [
    "# 3 - Test the keypoints from images\n",
    "\n",
    "- after we got the keypoints from the frame if we are feeling that those keypoints might be wrong we can use the below code to draw the extracted keypoints with the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7becbb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmark_from_array(image,keyPoints):\n",
    "    image_rows, image_cols, _ = image.shape\n",
    "    \n",
    "    \n",
    "    for i in range(len(keyPoints)//2):\n",
    "        x = keyPoints[i*2]\n",
    "        y = keyPoints[i*2+1]\n",
    "        if(x!=0 and y!=0): \n",
    "            landmark_px = mp_drawing._normalized_to_pixel_coordinates(x,y,\n",
    "                                                       image_cols, image_rows)\n",
    "            cv2.circle(image, landmark_px, 2, (0,0,255),\n",
    "                     30)\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ca38a4b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image num 0\n",
      "image num 1\n",
      "image num 2\n",
      "image num 3\n",
      "image num 4\n",
      "image num 5\n",
      "image num 6\n",
      "image num 7\n",
      "image num 8\n",
      "image num 9\n",
      "image num 10\n",
      "image num 11\n",
      "image num 12\n",
      "image num 13\n",
      "image num 14\n",
      "image num 15\n",
      "image num 16\n",
      "image num 17\n",
      "image num 18\n",
      "image num 19\n",
      "image num 20\n",
      "image num 21\n",
      "image num 22\n",
      "image num 23\n",
      "image num 24\n",
      "image num 25\n",
      "image num 26\n",
      "image num 27\n",
      "image num 28\n",
      "image num 29\n",
      "image num 30\n",
      "image num 31\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data)):\n",
    "    \n",
    "    print(f\"image num {i}\")\n",
    "    image = frames[i]\n",
    "    keypoint = data[i]\n",
    "\n",
    "    draw_landmark_from_array(image,keypoint)\n",
    "\n",
    "    image = cv2.resize(image, (640, 480)) \n",
    "    cv2.imshow(\"frame\",image)\n",
    "    if cv2.waitKey(500) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3309de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
